{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Processes on Molecules in GPflow\n",
    "\n",
    "This example demonstrates how it's possible to train a Gaussian Process to predict molecular properties using the GPflow library https://gpflow.readthedocs.io/en/master/ and a custom-defined Tanimoto kernel.\n",
    "\n",
    "In our example, we'll be trying to predict the experimentally-determined electronic transition wavelengths of molecular photoswitches, a class of molecule that undergoes a reversible transformation between its E and Z isomers upon the application of light. The figure below, created by the artistically indefatigable Aditya Thawani https://twitter.com/RaymondThawani who managed to feature his own hand in the logo for this repo, illustrates this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../photoswitching.png\" width=\"500\" title=\"logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing all of the machine learning and chemistry libraries we're going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "from gpflow.mean_functions import Constant\n",
    "from gpflow.utilities import positive, print_summary\n",
    "from gpflow.utilities.ops import broadcasting_elementwise\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit.Chem import AllChem, Descriptors, MolFromSmiles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As our molecular representation, we're going to be working with the widely-used Morgan fingerprints. Under this representation, molecules are represented as bit vectors which means that standard Gaussian Process kernels such as the squared exponential kernel or the Matern$\\frac{1}{2}$ kernel won't be ideal as they're designed for continuous spaces. We can however, design a custom \"Tanimoto\" or \"Jaccard\" kernel designed to compute similarity between bit vectors\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../equation_tanimoto.png\" width=\"400\" title=\"logo\">\n",
    "</p>\n",
    "\n",
    "where $\\textbf{x}$ and $\\textbf{x'}$ are bit vectors and $\\sigma$ is the kernel variance. It is relatively straightforward to define a custom Tanimoto kernel in GPflow cf. https://gpflow.readthedocs.io/en/master/notebooks/tailor/kernel_design.html. The definition below differs slightly because it is designed to compute the Tanimoto similarity over matrix input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanimoto(gpflow.kernels.Kernel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # We constrain the value of the kernel variance to be positive when it's being optimised\n",
    "        self.variance = gpflow.Parameter(1.0, transform=positive())\n",
    "\n",
    "    def K(self, X, X2=None):\n",
    "        \"\"\"\n",
    "        Compute the Tanimoto kernel matrix σ² * ((<x, y>) / (||x||^2 + ||y||^2 - <x, y>))\n",
    "\n",
    "        :param X: N x D array\n",
    "        :param X2: M x D array. If None, compute the N x N kernel matrix for X.\n",
    "        :return: The kernel matrix of dimension N x M\n",
    "        \"\"\"\n",
    "        if X2 is None:\n",
    "            X2 = X\n",
    "\n",
    "        Xs = tf.reduce_sum(tf.square(X), axis=-1)  # Squared L2-norm of X\n",
    "        X2s = tf.reduce_sum(tf.square(X2), axis=-1)  # Squared L2-norm of X2\n",
    "        outer_product = tf.tensordot(X, X2, [[-1], [-1]])  # outer product of the matrices X and X2\n",
    "\n",
    "        # Analogue of denominator in Tanimoto formula\n",
    "\n",
    "        denominator = -outer_product + broadcasting_elementwise(tf.add, Xs, X2s)\n",
    "\n",
    "        return self.variance * outer_product/denominator\n",
    "\n",
    "    def K_diag(self, X):\n",
    "        \"\"\"\n",
    "        Compute the diagonal of the N x N kernel matrix of X\n",
    "        :param X: N x D array\n",
    "        :return: N x 1 array\n",
    "        \"\"\"\n",
    "        return tf.fill(tf.shape(X)[:-1], tf.squeeze(self.variance))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read in our photoswitch molecules, represented as SMILES. The property we're predicting is the electronic transition wavelength of the E isomer of each molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/photoswitches.csv')  # Load the photoswitch dataset using pandas\n",
    "\n",
    "# Create a list of molecules smiles and associated properties\n",
    "smiles_list = df['SMILES'].to_list()\n",
    "property_vals = df['E isomer pi-pi* wavelength in nm'].to_numpy()\n",
    "\n",
    "# Delete NaN values\n",
    "smiles_list = list(np.delete(np.array(smiles_list), np.argwhere(np.isnan(property_vals))))\n",
    "y = np.delete(property_vals, np.argwhere(np.isnan(property_vals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert our molecules to 2048 bit Morgan fingerprints using a bond radius of 3. We denote the molecules as X and the property values by y. I guess this might make it easier to remember what the inputs and outputs are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdkit_mols = [MolFromSmiles(smiles) for smiles in smiles_list]\n",
    "X = [AllChem.GetMorganFingerprintAsBitVect(mol, radius=3, nBits=2048) for mol in rdkit_mols]\n",
    "X = np.asarray(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our optimisation objective for fitting the GP hyperparameters, namely the negative log marginal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the Gaussian Process Regression Model using the Tanimoto kernel\n",
    "\n",
    "m = None\n",
    "\n",
    "def objective_closure():\n",
    "    return -m.log_marginal_likelihood()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a utility function to standardise our output values. This is typically good practice before fitting a GP cf the \"Other Tricks\" slide in Iain Murray's presentation https://homepages.inf.ed.ac.uk/imurray2/teaching/08gp_slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Apply feature scaling, dimensionality reduction to the data. Return the standardised and low-dimensional train and\n",
    "    test sets together with the scaler object for the target values.\n",
    "\n",
    "    :param X_train: input train data\n",
    "    :param y_train: train labels\n",
    "    :param X_test: input test data\n",
    "    :param y_test: test labels\n",
    "    :return: X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, y_scaler\n",
    "    \"\"\"\n",
    "\n",
    "    x_scaler = StandardScaler()\n",
    "    X_train_scaled = x_scaler.fit_transform(X_train)\n",
    "    X_test_scaled = x_scaler.transform(X_test)\n",
    "    y_scaler = StandardScaler()\n",
    "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "    y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled, y_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the GP. We can inspect the learned kernel hyperparameters although these might not be so intuitive in the case of bit vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════════════════════════╤═══════════╤══════════════════╤═════════╤═════════════╤═════════╤═════════╤═══════════╕\n",
      "│ name                    │ class     │ transform        │ prior   │ trainable   │ shape   │ dtype   │     value │\n",
      "╞═════════════════════════╪═══════════╪══════════════════╪═════════╪═════════════╪═════════╪═════════╪═══════════╡\n",
      "│ GPR.mean_function.c     │ Parameter │                  │         │ True        │ ()      │ float64 │ 0.0443125 │\n",
      "├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤\n",
      "│ GPR.kernel.variance     │ Parameter │ Softplus         │         │ True        │ ()      │ float64 │ 0.463848  │\n",
      "├─────────────────────────┼───────────┼──────────────────┼─────────┼─────────────┼─────────┼─────────┼───────────┤\n",
      "│ GPR.likelihood.variance │ Parameter │ Softplus + Shift │         │ True        │ ()      │ float64 │ 0.0129625 │\n",
      "╘═════════════════════════╧═══════════╧══════════════════╧═════════╧═════════════╧═════════╧═════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_set_size, random_state=0)\n",
    "\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "#  We standardise the outputs but leave the inputs unchanged\n",
    "\n",
    "_, y_train, _, y_test, y_scaler = transform_data(X_train, y_train, X_test, y_test)\n",
    "\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "k = Tanimoto()\n",
    "m = gpflow.models.GPR(data=(X_train, y_train), mean_function=Constant(np.mean(y_train)), kernel=k, noise_variance=1)\n",
    "\n",
    "# Optimise the kernel variance and noise level by the marginal likelihood\n",
    "\n",
    "opt = gpflow.optimizers.Scipy()\n",
    "opt.minimize(objective_closure, m.trainable_variables, options=dict(maxiter=100))\n",
    "print_summary(m)\n",
    "\n",
    "# mean and variance GP prediction\n",
    "\n",
    "y_pred, y_var = m.predict_f(X_test)\n",
    "y_pred = y_scaler.inverse_transform(y_pred)\n",
    "y_test = y_scaler.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can output the train and test root-mean-square error (RMSE), mean absolute error (MAE) and $R^2$ value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train RMSE (Standardised): 0.036 nm\n",
      "Train RMSE: 2.422 nm\n",
      "\n",
      "Test R^2: 0.916\n",
      "Test RMSE: 17.997 nm\n",
      "Test MAE: 11.333 nm\n"
     ]
    }
   ],
   "source": [
    "# Output Standardised RMSE and RMSE on Train Set\n",
    "\n",
    "y_pred_train, _ = m.predict_f(X_train)\n",
    "train_rmse_stan = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "train_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(y_train), y_scaler.inverse_transform(y_pred_train)))\n",
    "print(\"\\nTrain RMSE (Standardised): {:.3f} nm\".format(train_rmse_stan))\n",
    "print(\"Train RMSE: {:.3f} nm\".format(train_rmse))\n",
    "\n",
    "\n",
    "# Output R^2, RMSE and MAE on the test set\n",
    "score = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nTest R^2: {:.3f}\".format(score))\n",
    "print(\"Test RMSE: {:.3f} nm\".format(rmse))\n",
    "print(\"Test MAE: {:.3f} nm\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! In our paper, we compared the predictions of the GP-Tanimoto model against those made by a cohort of human photoswitch chemists achieving lower test error in the case of all 5 molecules comprising the test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../humanGraph.png\" width=\"500\" title=\"logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAEs in this case are computed on a per molecule basis across all human participants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One defining characteristics of GPs is their ability to produce calibrated uncertainty estimates. In practical terms these uncertainty estimates can be helpful in capturing the model's confidence; if the test molecule is very different from the molecules seen in training the model can tell you! \n",
    "\n",
    "In programmatic terms, the test set uncertainties can be accessed by inspecting the variable y_var. One can obtain a ranked confidence list for the predictions by e.g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14 56 72 24 19  6 70 27 55 46 67 45 61  3 71 25 22 17 34  5 31 20 12 37\n",
      " 76  9 32 44 62 48 41 29 36 15 28 59 68 60  4  2 30 16  7 73 75 58 38 64\n",
      " 21 77 69 18 10 47  0 65 63 26 35 33  1 42 54 78 53 52 57 43 49 74 13 23\n",
      " 40 39 51 66  8 11 50]\n"
     ]
    }
   ],
   "source": [
    "ranked_confidence_list = np.argsort(y_var, axis=0).flatten()\n",
    "print(ranked_confidence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which outputs a list in which the molecules (represented by their test set index) are ranked by their prediction confidenceThe predict_with_GPR.py script offers further details. \n",
    "\n",
    "Graphically, it is possible to generate confidence-error curves in order to check that the uncertainties obtained by the GP are actually correlated with test set error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../confidence_curve.png\" width=\"500\" title=\"logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot above the x-axis, confidence percentile, may be obtained simply by ranking each model prediction of the test set in terms of the predictive variance at the location of that test input. As an example, molecules that lie in the 80th confidence percentile will be the 20% of test set molecules with the lowest model uncertainty. We then measure the prediction error at each confidence percentile across 200 random train/test splits to see whether the model’s confidence is correlated with the prediction error. We observe that the GP-Tanimoto model’s uncertainty estimates are positively correlated with prediction error.\n",
    "\n",
    "The practical takeaway from this plot is a proof of concept that model uncertainty can be incorporated into the decision process for selecting which photoswitch molecules to physically synthesise in the lab; if the predicted wavelength value of an \"unsynthesised\" molecule is desirable and the confidence in this prediction is high you might want to make it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice benefit of GPs; it is  possible to interpret the chemistry which different molecular representations capture by inspecting the learned kernel. See our paper for more details! https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"../triangleN.png\" width=\"500\" title=\"logo\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find this dataset or implementation please consider citing our paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "@article{Thawani2020,\n",
    "author = \"Aditya Thawani and Ryan-Rhys Griffiths and Arian Jamasb and Anthony Bourached and Penelope Jones and William McCorkindale and Alexander Aldrick and Alpha Lee\",\n",
    "title = \"{The Photoswitch Dataset: A Molecular Machine Learning Benchmark for the Advancement of Synthetic Chemistry}\",\n",
    "year = \"2020\",\n",
    "month = \"7\",\n",
    "url = \"https://chemrxiv.org/articles/preprint/The_Photoswitch_Dataset_A_Molecular_Machine_Learning_Benchmark_for_the_Advancement_of_Synthetic_Chemistry/12609899\",\n",
    "doi = \"10.26434/chemrxiv.12609899.v1\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as GPflow\n",
    "\n",
    "```\n",
    "@article{GPflow2020multioutput,\n",
    "  author = {{van der Wilk}, Mark and Dutordoir, Vincent and John, ST and\n",
    "            Artemev, Artem and Adam, Vincent and Hensman, James},\n",
    "  title = {A Framework for Interdomain and Multioutput {G}aussian Processes},\n",
    "  year = {2020},\n",
    "  journal = {arXiv:2003.01115},\n",
    "  url = {https://arxiv.org/abs/2003.01115}\n",
    "}\n",
    "```\n",
    "\n",
    "and the originators of the Tanimoto kernel:\n",
    "\n",
    "```\n",
    "@article{ralaivola2005graph,\n",
    "  title={Graph kernels for chemical informatics},\n",
    "  author={Ralaivola, Liva and Swamidass, Sanjay J and Saigo, Hiroto and Baldi, Pierre},\n",
    "  journal={Neural networks},\n",
    "  volume={18},\n",
    "  number={8},\n",
    "  pages={1093--1110},\n",
    "  year={2005},\n",
    "  publisher={Elsevier}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}